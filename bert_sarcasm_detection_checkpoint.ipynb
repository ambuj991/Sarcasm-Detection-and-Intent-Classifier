{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o16xRHm9S1NE"
      },
      "source": [
        "## Sarcasm Detection\n",
        "\n",
        "Sarcasm detection unlike sentiment analysis or simple text classification requires a lot of information about human interaction semantics, the model needs to know about how humans actually interact and it also needs to know the context in which certain words are used while being sarcastic and when not sarcastic.\n",
        "\n",
        "### In this notebook, we will try using the BERT classifier.\n",
        "\n",
        "## BERT - Bidirectional Encoder Representations from Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vWCMCpyS1NV"
      },
      "source": [
        "1. BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling.\n",
        "2. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training.\n",
        "3. The paper’s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models.\n",
        "4. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which was previously impossible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "IgoN5IMSS1NY"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6auBrd2S1Nc",
        "outputId": "47f7d6a0-36e4-4e44-a848-8cfaa6d47681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.8/dist-packages (1.8.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from wordcloud) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from wordcloud) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from wordcloud) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "e5qra1ElS1Ng"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
        "from keras.models import Model\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdiikqGpTN_2",
        "outputId": "02ac8000-7cdb-488c-8344-5f92243acb5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "q0kBIqnITSz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yK-N56S1Nl"
      },
      "source": [
        "## 1. Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Mull909rS1Nm",
        "outputId": "2b3df931-6dfb-43fd-f59d-448dc68cf895"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        article_link  \\\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
              "2  https://local.theonion.com/mom-starting-to-fea...   \n",
              "3  https://politics.theonion.com/boehner-just-wan...   \n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
              "\n",
              "                                            headline  is_sarcastic  \n",
              "0  former versace store clerk sues over secret 'b...             0  \n",
              "1  the 'roseanne' revival catches up to our thorn...             0  \n",
              "2  mom starting to fear son's web series closest ...             1  \n",
              "3  boehner just wants wife to listen, not come up...             1  \n",
              "4  j.k. rowling wishes snape happy birthday in th...             0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13e30810-8ad3-45be-a806-0ca2d2d05a7d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13e30810-8ad3-45be-a806-0ca2d2d05a7d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13e30810-8ad3-45be-a806-0ca2d2d05a7d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13e30810-8ad3-45be-a806-0ca2d2d05a7d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "df = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HTGIlVlS1No"
      },
      "outputs": [],
      "source": [
        "df.drop('article_link', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psJBiUWnS1Nq"
      },
      "source": [
        "## 2. Basic Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50tyIYX1S1Nr",
        "outputId": "8b23f871-bf9d-4eff-f07e-a7c5d23dbf98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5Aik_ScS1Nt"
      },
      "outputs": [],
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luHeGQBcS1Nu"
      },
      "outputs": [],
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
        "            final_text.append(i.strip().lower())\n",
        "    return \" \".join(final_text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['headline']=df['headline'].apply(denoise_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW72IxABS1Nv"
      },
      "source": [
        "## 3. Creating the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb_tJrLwS1Nw",
        "outputId": "ab8f2387-3e55-4010-cfa1-76faba500aaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['former', 'versace', 'store', 'clerk', 'sues']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "def get_corpus(text):\n",
        "    words = []\n",
        "    for i in text:\n",
        "        for j in i.split():\n",
        "            words.append(j.strip())\n",
        "    return words\n",
        "corpus = get_corpus(df.headline)\n",
        "corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2h4NHhjS1Nx",
        "outputId": "8e1777c0-e0cf-4f00-a160-2379a057e571"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'new': 1485,\n",
              " 'man': 1229,\n",
              " 'trump': 1157,\n",
              " 'one': 487,\n",
              " 'area': 484,\n",
              " 'donald': 468,\n",
              " 'says': 450,\n",
              " 'woman': 420,\n",
              " 'day': 391,\n",
              " 'first': 389}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "from collections import Counter\n",
        "counter = Counter(corpus)\n",
        "most_common = counter.most_common(10)\n",
        "most_common = dict(most_common)\n",
        "most_common"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhdH2nouS1Ny"
      },
      "source": [
        "## 4. Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UxrTzv-S1Nz"
      },
      "outputs": [],
      "source": [
        "X = df['headline']\n",
        "y = df['is_sarcastic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGIe_GVvS1Nz"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 0 , stratify = y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XuUyswhS1N0"
      },
      "outputs": [],
      "source": [
        "## pip install transformers\n",
        "## If transformers library is not installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjT-vTK7S1N0"
      },
      "source": [
        "#### DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bert’s performances as measured on the GLUE language understanding benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cCuJYUdS1N1",
        "outputId": "95a4bd50-b235-45d4-c36f-82bf5c4bb2bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "# First load the real tokenizer\n",
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n",
        "# Save the loaded tokenizer locally\n",
        "tokenizer.save_pretrained('.')\n",
        "# Reload it with the huggingface tokenizers library\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\n",
        "fast_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM5BjmwcS1N2"
      },
      "source": [
        "Before any sentence is passed into the BERT model, it has to go through some tokenization steps.\n",
        "1. Firstly, tokenize all words in the sentence corpus\n",
        "2. Next, all the tokens are embedded using the learned embeddings from a WordPiece Tokenizer. For each word we get 786 dim. vector representation.\n",
        "3. Segment Embedding - Helps in distinguishing the words belonging to different sentences.\n",
        "4. Positional Embedding - This is used to provide the positional understanding for each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4KSahgJS1N3"
      },
      "source": [
        "![](https://miro.medium.com/max/875/0*m_kXt3uqZH9e7H4w.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85lmQsY2S1N4"
      },
      "outputs": [],
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n",
        "\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(length=maxlen)\n",
        "\n",
        "    all_ids = []\n",
        "\n",
        "    for i in range(0, len(texts), chunk_size):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "\n",
        "    return np.array(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29F2BsJ2S1N-"
      },
      "outputs": [],
      "source": [
        "x_train = fast_encode(X_train.values, fast_tokenizer)\n",
        "x_test = fast_encode(X_test.values, fast_tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92gAfmKgS1N_"
      },
      "source": [
        "We shall be using the pre-trained BERT model which has been extensively trained on huge text corpuses of Wikipedia.\n",
        "Now after getting the pre-trained model, we can fine-tune the model for our own specific task.\n",
        "\n",
        "The key thing to notice is here is that for our final prediction we would only be using the output generated by the first token ([CLS]), after which we use a sigmoid activation for final prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMClCJf9S1N_"
      },
      "outputs": [],
      "source": [
        "def build_model(transformer, max_len=400):\n",
        "\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "on0ogxSwWHlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97mLKohnS1OA"
      },
      "source": [
        "Downloading the pre-trained BERT model.\n",
        "We can use either the CASED or the UNCASED version for model building.\n",
        "\n",
        "To explain the BERT training in simple words:\n",
        "The BERT pre-training consists of two steps which happen parallely, these include:\n",
        "### 1. Masked Language Model\n",
        "In this the model is provided with sentences and some of the words are masked with a [MASK] tag (15% of all words), the model is then asked to predict those masked words. The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness.\n",
        "\n",
        "### 2. Next Sentence Prediction\n",
        "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. This helps the model to understand the relationships between"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYUTSFwVS1OA"
      },
      "outputs": [],
      "source": [
        "# bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfVduUewS1OB"
      },
      "outputs": [],
      "source": [
        "model = build_model(bert_model, max_len=400)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oDecYAVS1OC"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train,y_train,batch_size = 16 ,validation_data=(x_test,y_test),epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA_29SqMS1OD"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSFeilmPS1OD"
      },
      "outputs": [],
      "source": [
        "epochs = [i for i in range(3)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_acc = history.history['val_accuracy']\n",
        "val_loss = history.history['val_loss']\n",
        "fig.set_size_inches(20,10)\n",
        "\n",
        "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
        "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\n",
        "ax[0].set_title('Training & Testing Accuracy')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "ax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\n",
        "ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\n",
        "ax[1].set_title('Training & Testing Loss')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJdrbuvLS1OE"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(x_test)\n",
        "pred[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wn4-d9lS1OE"
      },
      "outputs": [],
      "source": [
        "pred = np.round(pred).astype(int)\n",
        "pred[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP0aMIJQS1OF"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FWKnKt8S1OF"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test,pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFQqh9H2S1OG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH700rB5S1OG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}